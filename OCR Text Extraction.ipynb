{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13997897,"sourceType":"datasetVersion","datasetId":8920218},{"sourceId":14000173,"sourceType":"datasetVersion","datasetId":8921051}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **converting dataset to csv**","metadata":{}},{"cell_type":"code","source":"!pip install pillow pytesseract pandas tqdm scikit-learn --quiet\n!apt-get update >/dev/null 2>&1 && apt-get install -y tesseract-ocr >/dev/null 2>&1\n\nimport os\nfrom pathlib import Path\nfrom PIL import Image, ImageOps, ImageSequence\nimport pytesseract\nimport pandas as pd\nfrom tqdm import tqdm\nfrom IPython.display import FileLink, display\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# ---------- OCR helper ----------\ndef ocr_tiff(path, resize_max=2500, preprocess=True):\n    try:\n        img = Image.open(path)\n    except Exception:\n        return \"\", False\n    pages = []\n    for p in ImageSequence.Iterator(img):\n        p = p.convert(\"RGB\")\n        if max(p.size) > resize_max:\n            scale = resize_max / max(p.size)\n            p = p.resize((int(p.width*scale), int(p.height*scale)), Image.LANCZOS)\n        if preprocess:\n            p = ImageOps.grayscale(p)\n        try:\n            txt = pytesseract.image_to_string(p, config=\"--psm 3\")\n        except Exception:\n            txt = pytesseract.image_to_string(p)\n        pages.append(txt.strip())\n    return \"\\n\\n\".join([t for t in pages if t]), True\n\n# ---------- collect rows from a directory of label subfolders ----------\ndef rows_from_directory(root_dir, exts=(\".tif\", \".tiff\"), save_texts=False, texts_out_dir=\"/kaggle/working/texts\"):\n    root = Path(root_dir)\n    if not root.exists() or not root.is_dir():\n        raise FileNotFoundError(f\"Directory not found: {root_dir}\")\n    rows = []\n    labels = sorted([d for d in root.iterdir() if d.is_dir()])\n    if not labels:\n        raise SystemExit(f\"No label subfolders found under {root_dir}. Each class must be a subfolder.\")\n    if save_texts:\n        Path(texts_out_dir).mkdir(parents=True, exist_ok=True)\n    for lab in labels:\n        tiffs = [p for p in lab.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n        for f in tqdm(sorted(tiffs), desc=f\"Processing {lab.name}\", unit=\"file\"):\n            text, ocr_used = ocr_tiff(f)\n            rows.append({\n                \"filepath\": str(f.resolve()),\n                \"label\": lab.name,\n                \"filetype\": f.suffix.lower().lstrip(\".\"),\n                \"text\": text,\n                \"ocr_used\": bool(ocr_used)\n            })\n            if save_texts:\n                outname = Path(texts_out_dir) / (f.stem + \".txt\")\n                try:\n                    outname.write_text(text or \"\", encoding=\"utf-8\")\n                except Exception:\n                    pass\n    return rows\n\n# ---------- MAIN ----------\nprint(\"Provide full paths to your TRAIN / VAL / TEST directories (each contains label subfolders).\")\ntrain_dir = input(\"TRAIN directory : \").strip()\nval_dir   = input(\"VAL directory : \").strip()\ntest_dir  = input(\"TEST  directory : \").strip()\nsave_texts_ans = input(\"Save per-document .txt files for inspection? (y/N): \").strip().lower()\nsave_texts = save_texts_ans == \"y\"\n\nif not train_dir:\n    raise SystemExit(\"TRAIN directory is required. Provide the full path (e.g. /kaggle/input/your-dataset/train).\")\n\n# Process TRAIN\nprint(\"\\nProcessing TRAIN directory:\", train_dir)\nrows_train = rows_from_directory(train_dir, save_texts=save_texts)\nfor r in rows_train: r[\"_source_split\"] = \"train\"\n\n# Process VAL if given; otherwise will be created from TRAIN rows\nrows_val = []\nif val_dir:\n    print(\"\\nProcessing VAL directory:\", val_dir)\n    rows_val = rows_from_directory(val_dir, save_texts=save_texts)\n    for r in rows_val: r[\"_source_split\"] = \"val\"\nelse:\n    print(\"\\nNo VAL directory provided — will create stratified 10% validation from TRAIN after OCR.\")\n\n# Process TEST if given\nrows_test = []\nif test_dir:\n    print(\"\\nProcessing TEST directory:\", test_dir)\n    rows_test = rows_from_directory(test_dir, save_texts=save_texts)\n    for r in rows_test: r[\"_source_split\"] = \"test\"\n\n# Combine and possibly split\ndf_train = pd.DataFrame(rows_train)\ndf_val = pd.DataFrame(rows_val) if len(rows_val) else pd.DataFrame(columns=df_train.columns)\ndf_test = pd.DataFrame(rows_test) if len(rows_test) else pd.DataFrame(columns=df_train.columns)\n\n# If val not provided, create stratified split from df_train\nif df_val.empty:\n    # drop empty OCR rows before split\n    df_nonempty = df_train[df_train[\"text\"].str.strip() != \"\"].copy()\n    if df_nonempty.empty:\n        print(\"Warning: OCR produced empty text for all TRAIN files; val split skipped.\")\n    else:\n        # we want 10% of original train as validation\n        try:\n            train_df, val_df = train_test_split(df_nonempty, test_size=0.10, stratify=df_nonempty[\"label\"], random_state=42)\n        except Exception:\n            # fallback to random split without stratify if some classes have too few examples\n            train_df, val_df = train_test_split(df_nonempty, test_size=0.10, random_state=42)\n        df_train = train_df.reset_index(drop=True)\n        df_val = val_df.reset_index(drop=True)\n        # Note: if some rows were empty and removed above, they are not in train/val — you can inspect /kaggle/working/manifest.csv\n\n# Save CSVs to /kaggle/working\ntrain_out = \"/kaggle/working/train.csv\"\nval_out   = \"/kaggle/working/val.csv\"\ntest_out  = \"/kaggle/working/test.csv\"\nmanifest_out = \"/kaggle/working/manifest.csv\"\n\nif not df_train.empty:\n    df_train = df_train.drop(columns=[\"_source_split\"], errors=\"ignore\")\n    df_train.to_csv(train_out, index=False, encoding=\"utf-8\")\n    print(f\"\\nSaved: {train_out}  rows: {df_train.shape[0]}\")\n    display(FileLink(train_out))\nelse:\n    print(\"\\nNo train rows to save.\")\n\nif not df_val.empty:\n    df_val = df_val.drop(columns=[\"_source_split\"], errors=\"ignore\")\n    df_val.to_csv(val_out, index=False, encoding=\"utf-8\")\n    print(f\"Saved: {val_out}  rows: {df_val.shape[0]}\")\n    display(FileLink(val_out))\nelse:\n    print(\"No val rows to save.\")\n\nif not df_test.empty:\n    df_test = df_test.drop(columns=[\"_source_split\"], errors=\"ignore\")\n    df_test.to_csv(test_out, index=False, encoding=\"utf-8\")\n    print(f\"Saved: {test_out}  rows: {df_test.shape[0]}\")\n    display(FileLink(test_out))\nelse:\n    print(\"No test rows to save (test dir was not provided).\")\n\n# Combined manifest (all rows)\ndf_all = pd.concat([df_train.assign(_source=\"train\"), df_val.assign(_source=\"val\"), df_test.assign(_source=\"test\")], ignore_index=True, sort=False)\ndf_all.to_csv(manifest_out, index=False, encoding=\"utf-8\")\nprint(f\"\\nSaved manifest: {manifest_out}  total rows: {df_all.shape[0]}\")\ndisplay(FileLink(manifest_out))\n\nprint(\"\\nDone — CSVs are in /kaggle/working/.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kaggle-ready: generate ner_train.csv & ner_val.csv from provided train.csv & val.csv\n# Paste into a Kaggle notebook cell and run.\n\n!pip install -q spacy pandas scikit-learn tqdm\n!python -m spacy download en_core_web_sm >/dev/null 2>&1\n\nimport re\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport spacy\n\n# --------------- CONFIG ---------------\nTRAIN_CSV = \"/kaggle/working/train.csv\"   # must exist and contain 'text' column\nVAL_CSV   = \"/kaggle/working/val.csv\"     # must exist and contain 'text' column\nOUT_TRAIN = \"ner_train.csv\"\nOUT_VAL   = \"ner_val.csv\"\nSPACY_MODEL = \"en_core_web_sm\"\nBATCH_SIZE = 16\n# --------------------------------------\n\n# checks\nif not Path(TRAIN_CSV).exists():\n    raise SystemExit(f\"{TRAIN_CSV} not found. Upload train.csv to the working directory.\")\nif not Path(VAL_CSV).exists():\n    raise SystemExit(f\"{VAL_CSV} not found. Upload val.csv to the working directory.\")\n\ndf_train = pd.read_csv(TRAIN_CSV)\ndf_val   = pd.read_csv(VAL_CSV)\n\nif \"text\" not in df_train.columns:\n    raise SystemExit(\"train.csv must contain a 'text' column.\")\nif \"text\" not in df_val.columns:\n    raise SystemExit(\"val.csv must contain a 'text' column.\")\n\n# load spaCy and ensure sentence segmentation\nnlp = spacy.load(SPACY_MODEL)\nif \"sentencizer\" not in nlp.pipe_names:\n    nlp.add_pipe(\"sentencizer\")\n\n# regex rules to augment spaCy NER (common doc fields)\nemail_re    = re.compile(r\"[A-Za-z0-9+_.-]+@[A-Za-z0-9.-]+\")\nphone_re    = re.compile(r\"(\\+?\\d{1,3}[-\\s]?)?(\\d{10}|\\d{3}[-\\s]\\d{3}[-\\s]\\d{4})\")\ninvoice_re  = re.compile(r\"(INV[-_\\s]?\\d+|Invoice\\s*(No|#|Number)[:\\s]*[A-Za-z0-9\\-_/]+)\", flags=re.I)\nmoney_re    = re.compile(r\"([₹$€£]\\s?\\d[\\d,]*(?:\\.\\d{1,2})?)\")\n\ndef token_indices_from_span(start, end, tokens):\n    \"\"\"Return token indices in tokens list that overlap char span [start,end).\"\"\"\n    idxs = []\n    for i, tok in enumerate(tokens):\n        ts, te = tok.idx, tok.idx + len(tok.text)\n        if te <= start:\n            continue\n        if ts >= end:\n            break\n        idxs.append(i)\n    return idxs\n\ndef doc_to_bio_rows(doc, sent_start_id=0):\n    \"\"\"\n    Convert a spaCy Doc to BIO token rows with regex overrides.\n    Returns (rows, last_sentence_id)\n    \"\"\"\n    rows = []\n    sid = sent_start_id\n\n    # build override spans (absolute char positions) with labels\n    overrides = {}\n    for rgx, lab in [(email_re, \"EMAIL\"), (phone_re, \"PHONE\"), (invoice_re, \"INVOICE_ID\"), (money_re, \"MONEY\")]:\n        for m in rgx.finditer(doc.text):\n            overrides[(m.start(), m.end())] = lab\n\n    for sent in doc.sents:\n        tokens = list(sent)\n        labels = [\"O\"] * len(tokens)\n\n        # spaCy entities first\n        for ent in sent.ents:\n            ent_idxs = token_indices_from_span(ent.start_char, ent.end_char, tokens)\n            if ent_idxs:\n                labels[ent_idxs[0]] = f\"B-{ent.label_}\"\n                for j in ent_idxs[1:]:\n                    labels[j] = f\"I-{ent.label_}\"\n\n        # apply regex overrides (may add/override labels)\n        for (s,e), lab in overrides.items():\n            if e <= sent.start_char or s >= sent.end_char:\n                continue\n            rel_start = max(s, sent.start_char)\n            rel_end   = min(e, sent.end_char)\n            ov_idx = token_indices_from_span(rel_start, rel_end, tokens)\n            if ov_idx:\n                labels[ov_idx[0]] = f\"B-{lab}\"\n                for j in ov_idx[1:]:\n                    labels[j] = f\"I-{lab}\"\n\n        # append token rows for this sentence\n        for tok, lab in zip(tokens, labels):\n            rows.append((sid, tok.text, lab))\n        sid += 1\n\n    last_sid = sid - 1 if sid > sent_start_id else sent_start_id\n    return rows, last_sid\n\ndef annotate_texts(texts, start_sid=0):\n    all_rows = []\n    sid = start_sid\n    for doc in nlp.pipe(texts, batch_size=BATCH_SIZE):\n        rows, last = doc_to_bio_rows(doc, sent_start_id=sid)\n        all_rows.extend(rows)\n        sid = last + 1\n    return all_rows, sid\n\n# Annotate TRAIN\ntrain_texts = df_train[\"text\"].astype(str).tolist()\nprint(f\"Annotating {len(train_texts)} training documents ...\")\ntrain_rows, next_sid = annotate_texts(train_texts, start_sid=0)\n\n# Annotate VAL (start sentence ids after train to keep unique sentence ids across datasets)\nval_texts = df_val[\"text\"].astype(str).tolist()\nprint(f\"Annotating {len(val_texts)} validation documents ...\")\nval_rows, _ = annotate_texts(val_texts, start_sid=next_sid+1)\n\n# Save CSVs\ndf_ner_train = pd.DataFrame(train_rows, columns=[\"sentence_id\",\"word\",\"label\"])\ndf_ner_val   = pd.DataFrame(val_rows, columns=[\"sentence_id\",\"word\",\"label\"])\n\ndf_ner_train.to_csv(OUT_TRAIN, index=False)\ndf_ner_val.to_csv(OUT_VAL, index=False)\n\nprint(f\"\\nSaved {OUT_TRAIN} ({len(df_ner_train)} rows) and {OUT_VAL} ({len(df_ner_val)} rows).\")\nprint(\"\\nSample (ner_train.csv):\")\nprint(df_ner_train.head(30).to_string(index=False))\n\nprint(\"\\nDone — open these CSVs in Doccano/LabelStudio to correct labels before training NER for best results.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T11:32:35.862357Z","iopub.status.idle":"2025-12-05T11:32:35.862580Z","shell.execute_reply.started":"2025-12-05T11:32:35.862473Z","shell.execute_reply":"2025-12-05T11:32:35.862482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}